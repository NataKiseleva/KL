# -*- coding: utf-8 -*-
"""SVM _Kiseleva.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QXSQmv9qd7VLyYNZQm_qHUdE6PXvHzB6
"""

import numpy as np
import scipy.optimize
import matplotlib.pyplot as plt

def f(x):
  return x**2

def df(x):
  return 2*x

print(scipy.optimize.minimize(f, np.random.randint(-1000, 1000), jac=df))

print(scipy.optimize.minimize(f, np.random.randint(-1000, 1000), jac=False))

"""the first faster. first: 3.2ms second: 7.16 ms
first: function, second: developer
"""

def hinge_loss_surrogate(y_gold, y_pred):
  return max(0, 1 - y_gold * y_pred)
def svm_loss(p, C, D):
  w, b = p[:-1],  p[-1]
  res = 0.5*np.dot(w, w) + C*sum(hinge_loss_surrogate(pairs[1], np.dot(w,pairs[0])+b) for pairs in D)
  return res

def svm(D, C=1):
  w = np.zeros(len(D[0][0]))
  b = np.array([0])
  function = lambda P: svm_loss(P, C=C, D=D)
  result = scipy.optimize.minimize(function, x0=np.concatenate([w,b]), jac=False).x
  ow = result[:-1]
  ob = result[-1]
  return ow, ob

def gradient_hinge_loss_surrogate(y_gold, y_pred):
  if hinge_loss_surrogate(y_gold, y_pred) == 0:
    return 0
  else:
    return -y_gold

def gradient_svm_loss(p, D, C=1):
  w, b = p[:-1],  p[-1]
  d_w = w + C*sum(gradient_hinge_loss_surrogate(pairs[1], np.dot(w,pairs[0])+b)*pairs[0] for pairs in D)
  d_b = C*sum(gradient_hinge_loss_surrogate(pairs[1], np.dot(w,pairs[0])+b) for pairs in D)
  return np.hstack([d_w,d_b])

def svm(D, use_gradient=False, C=1):
  w = np.random.randint(100, size=len(D[0][0]))
  b = np.array([0])
  if use_gradient:
    function = lambda P: svm_loss(P, C=C, D=D)
    function_o  = lambda P: gradient_svm_loss(P, C=C, D=D)
    result = scipy.optimize.minimize(function, x0=np.concatenate([w,b]), jac=function_o).x
    ow = result[:-1]
    ob = result[-1]
  else:
    function = lambda P: svm_loss(P, C=C, D=D)
    result = scipy.optimize.minimize(function, x0=np.concatenate([w,b]), jac=False).x
    ow = result[:-1]
    ob = result[-1]
  return ow, ob

x_plus = np.random.normal(loc=[-1,-1], scale=0.5, size=(20,2))
x_minus = np.random.normal(loc=[1,1], scale=0.5, size=(20,2))

x_y = [(x, 1) for x in x_plus]
x_y_m = [(x, -1) for x in x_minus]

D =  x_y + x_y_m

plt.scatter(
	x_plus[:,0], x_plus[:,1],
	marker='+',
	color='blue'
)
plt.scatter(
	x_minus[:,0], x_minus[:,1],
	marker='x',
	color='red'
)

def line(w, b):
  x = [-3, 3]
  y = [-(w[0]*x[0] + b)/w[1], -(w[0]*x[1] + b)/w[1]]
  print(y)
  return plt.plot(x,y)

def AveragedPerceptronTrain(D, maxiter = 100):
  w = np.zeros(len(D[0][0]))
  b = np.zeros(1)
  u = np.zeros(len(D[0][0]))
  beta = 0
  c = 1
  for i in range(maxiter):
    for x, y in D:
      a = np.dot(x,w) + b
      if np.sign(y*a) <= 0: 
        w += y*x
        b += y
        u += y*c*x
        beta += y*c
      c += 1
  return w-(1/c)*u, b-beta*(1/c)

w_s, b_s = svm(D)

w_s_g, b_s_g = svm(D, use_gradient=True)

w_p, b_p = AveragedPerceptronTrain(D)

_ = plt.scatter(x_plus[:,0], x_plus[:,1],	marker='+',	color='blue')
_ = plt.scatter(x_minus[:,0], x_minus[:,1],	marker='x',	color='red')
_ = line(w_s, b_s)
_ = line(w_s_g, b_s_g)
_ = line(w_p, b_p)
plt.savefig("svm-svm-perceptron.pdf") 
plt.show()